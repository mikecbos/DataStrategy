{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c00b7296-6ec3-4831-8d19-0fc154da8014",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "> #  Data Analysis & Visualization &nbsp;- Designed by Michael Cheng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf239ee6-096a-4cc7-9099-c807a854245a",
   "metadata": {},
   "source": [
    "## Project Problem Statement - Auto-mpg Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9949d040-692c-44ef-a9f2-b89e28082f8f",
   "metadata": {},
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016e175f-6573-44a2-8888-a4777d51ac65",
   "metadata": {},
   "source": [
    "**Context**\n",
    "\n",
    "The shifting market conditions, globalization, cost pressure, and volatility are leading to a change in the automobile market landscape. The emergence of data, in conjunction with machine learning in automobile companies, has paved a way that is helping bring operational and business transformations.\n",
    "\n",
    "The automobile market is vast and diverse, with numerous vehicle categories being manufactured and sold with varying configurations of attributes such as displacement, horsepower, and acceleration. We aim to find combinations of these features that can clearly distinguish certain groups of automobiles from others through this analysis, as this will inform other downstream processes for any organization aiming to sell each group of vehicles to a slightly different target audience.\n",
    "\n",
    "You are a Data Scientist at SecondLife which is a leading used car dealership with numerous outlets across the US. Recently, <span style=\"background-color: yellow\">they have started shifting their focus to vintage cars and have been diligently collecting data about all the vintage cars they have sold over the years. The Director of Operations at SecondLife wants to leverage the data to extract insights about the cars and find different groups of vintage cars to target the audience more efficiently.</span> [1]\n",
    "\n",
    "**Objective**\n",
    "\n",
    "<span style=\"background-color: yellow\">The objective of this problem is to explore the data, extract meaningful insights, and find different groups of vehicles in the data by using dimensionality reduction techniques like PCA and t-SNE.</span>\n",
    "\n",
    "**Data Description:**\n",
    "There are 8 variables in the dataset:\n",
    "\n",
    "* mpg: miles per gallon\n",
    "\n",
    "* cyl: number of cylinders\n",
    "\n",
    "* disp: engine displacement (cu. inches) or engine size\n",
    "\n",
    "* hp: horsepower\n",
    "  \n",
    "* wt: vehicle weight (lbs.)\n",
    "  \n",
    "* acc: time taken to accelerate from 0 to 60 mph (sec.)\n",
    "\n",
    "* yr: model year\n",
    "  \n",
    "* car name: car model name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a1a445-738e-4037-a1c3-155aee10e762",
   "metadata": {},
   "source": [
    "### Import Libraries & Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8d874f-f275-4b23-8ae5-81423de9052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Importing PCA and t-SNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Summary Tools\n",
    "from summarytools import dfSummary\n",
    "data = pd.read_csv(\"auto-mpg.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb56f9-34f9-496e-a22b-7f994ce68537",
   "metadata": {},
   "source": [
    "### DataPreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcefe5e-1e33-4123-a716-1d1248742562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of data\n",
    "df = data.copy()\n",
    "\n",
    "# Overview of data\n",
    "print(df.head())\n",
    "df.info()\n",
    "dfSummary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1aa834-6336-467a-95b7-f95638fc3a7b",
   "metadata": {},
   "source": [
    "#### The overview above shows: \n",
    "\n",
    "1. There are no duplicates or missing values.\n",
    "2. The average model year is 1976, with a relatively low spread (3.7 years).\n",
    "3. The minimum year is 1970, the median is 1976, and the maximum is 1982, showing a symmetric spread around the median.\n",
    "4. With an Inter-Quartile Range (IQR) at 6.0; this suggests the middle 50% of model years span 6 years (i.e. from 1973 to 1979, depending on the exact quartiles).\n",
    "5. Coefficient of Variation (CV) shows a relative variability of 20.6%; this shows a moderately high standard deviation at about 20.6% of the mean, indicating reasonable variability in model years\n",
    "6. The dataset represents a vehicle's **Performance Features** in the following:\n",
    "   \n",
    "- mpg\n",
    "- cylinders\n",
    "- displacement\n",
    "- horsepower\n",
    "- weight\n",
    "- acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319f5b7d-b1bf-470f-9cf8-373a70d1236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objects Columns: Review 'car name'\n",
    "df['car name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ec1ad2-0746-4cc1-a219-f0e69e676c66",
   "metadata": {},
   "source": [
    "#### Decision Point: \n",
    "- Because the firm has *\"recently started\"* [[1]](#Background_1) shifting their attention to vintage vehicles, this dataset will contain vintage *and non-vintage* vehicles. \n",
    "- Although \"vintage\" may literally be designated to a particular year rather than the particular make/model of a vehicle, there is also a significant factor from *cultural perception* that plays into a vehicle's **vintage value**.\n",
    "- Since the firm desires to \"leverage the data to extract insights about the cars and find different groups of vintage cars to target the audience more efficiently\" [[1]](#Background_1), 'car name' will be considered along with other features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852e2133-75d9-4cd4-9549-20817bf7189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objects Columns: Review 'horsepower' \n",
    "df['horsepower'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c0828a-1b78-4fe8-b7bd-6b348f8c04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undefined value \"?\" occurence\n",
    "print(\"Instances of ? in 'horsepower'\")\n",
    "df['horsepower'].value_counts()['?']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf5d4fa-f357-482e-a6ff-a256ef6b2081",
   "metadata": {},
   "source": [
    "#### Decision Point: \n",
    "- \"Horsepower\" should be converted to a numeric data type for meaningful analysis and visualization\n",
    "- \"?\" values determination: Use Regression (generalization based on global patterns) rather than KNN accounting for variabilities based on local patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc498d-db92-476a-878a-c2d83e4e7a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Regression to predict \"?\" horsepower values\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Replace '?' with NaN and convert to numeric\n",
    "df['horsepower'] = pd.to_numeric(df['horsepower'].replace('?', np.nan), errors='coerce')\n",
    "\n",
    "# Split rows with and without missing horsepower\n",
    "df_missing_hp = df[df['horsepower'].isna()]\n",
    "df_non_missing_hp = df[~df['horsepower'].isna()]\n",
    "\n",
    "# Features and target for non-missing rows\n",
    "X = df_non_missing_hp[['mpg', 'cylinders', 'displacement', 'weight', 'acceleration', 'model year']]\n",
    "y = df_non_missing_hp['horsepower']\n",
    "\n",
    "# Train a regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict missing horsepower\n",
    "X_missing = df_missing_hp[['mpg', 'cylinders', 'displacement', 'weight', 'acceleration', 'model year']]\n",
    "df.loc[df['horsepower'].isna(), 'horsepower'] = model.predict(X_missing)\n",
    "\n",
    "# Display updated DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816e5478-08ac-47dd-9b34-cde26938d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "df['horsepower'].isna().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b087f9-c6c2-44d9-9314-bdd848b82558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review unique values\n",
    "df['horsepower'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c994c12-06a7-45d5-97cf-88ec118b09e9",
   "metadata": {},
   "source": [
    "##### Decision Point\n",
    "There are a few instances of trailing decimals that will be distracting for display purposes. This feature, however, will require domain experts to validate. Moreover, Vintage Classification is not primarily determined by **horsepower**, or any other *performance features* (see [above](#The_overview_above_shows:)). Thus, it seems better to leave these records as flagged entries by virtue of their trailing decimals, and allow domain experts to review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c407b1af-8c7b-4439-95dc-7f55f9822ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index column for visualization traceability\n",
    "df['index'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb0099a-a60a-48bc-bdc5-22b81fbca84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review preprocessed data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac875a4-be7f-4928-b369-76a98121aaa9",
   "metadata": {},
   "source": [
    "#### Observations: \n",
    "* Make and Model, as discused above, can be sigificant in the determination of a vehicle's vintage classification\n",
    "* Most (89%) of the 'car name' feature is singular (designated as \"Other\" in frequency distribution)[[2]](#DataPreprocessing_1), which can be the very reason a vehicle could be classifed as \"vintage\"\n",
    "* Therefore, these 'car name' labels should be further reviewed, and disaggregated if possible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e7b66c-69d6-486a-96ab-26bfb263f145",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis and Hypothesis Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587b12b5-328a-4068-b1a5-4a5aaa4ac183",
   "metadata": {},
   "source": [
    "#### t-SNE_preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09734cfb-8661-4cc7-9839-d407c632b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE analysis to review car name\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "\n",
    "# Future warning suppressed\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "# Prepare t-SNE data\n",
    "features = df.drop('car name', axis=1)  \n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(features)-1))\n",
    "features_tsne = tsne.fit_transform(features)\n",
    "\n",
    "# Create DataFrame\n",
    "tsne_df = pd.DataFrame(features_tsne, columns=['t-SNE 1', 't-SNE 2'])\n",
    "tsne_df['car name'] = df['car name']\n",
    "\n",
    "# Plotly visualization with tuple for color parameter\n",
    "fig = px.scatter(tsne_df, x='t-SNE 1', y='t-SNE 2', \n",
    "                 color='car name', \n",
    "                 hover_data=['car name'],\n",
    "                 title='t-SNE Visualization of car name')\n",
    "\n",
    "fig.update_layout(\n",
    "   title='t-SNE Visualization of Car Name',\n",
    "   autosize=False,\n",
    "   width=800,\n",
    "   height=300,\n",
    "   xaxis=dict(\n",
    "       title='',\n",
    "       showticklabels=False, \n",
    "       showgrid=False, \n",
    "       zeroline=False\n",
    "   ),\n",
    "   yaxis=dict(\n",
    "       title='',\n",
    "       showticklabels=False, \n",
    "       showgrid=False, \n",
    "       zeroline=False\n",
    "   )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4dff59-0551-477f-bc83-48e47d2e8e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE to review only \"Other\" in car name\n",
    "\n",
    "# Identify car names with a frequency of 1\n",
    "singular_names = df['car name'].value_counts()[df['car name'].value_counts() == 1].index\n",
    "\n",
    "# Filter the dataset to include only those car names\n",
    "singular_df = df[df['car name'].isin(singular_names)]\n",
    "\n",
    "# Drop 'car name' for t-SNE, use other features for singular entries only\n",
    "features_singular = singular_df.drop('car name', axis=1)\n",
    "\n",
    "# Apply t-SNE to singular entries\n",
    "tsne_singular = TSNE(n_components=2, random_state=42, perplexity=min(30, len(features_singular)-1))\n",
    "features_tsne_singular = tsne_singular.fit_transform(features_singular)\n",
    "\n",
    "# Create DataFrame for Plotly\n",
    "tsne_singular_df = pd.DataFrame(features_tsne_singular, columns=['t-SNE 1', 't-SNE 2'])\n",
    "tsne_singular_df['car name'] = singular_df['car name'].values\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "# Plotly visualization\n",
    "fig = px.scatter(tsne_singular_df, x='t-SNE 1', y='t-SNE 2', \n",
    "                 color='car name', \n",
    "                 hover_data=['car name'],  # Hover over points to see car name\n",
    "                 title='t-SNE Visualization of Singular Frequency Car Names')\n",
    "\n",
    "# Update layout for cleaner visualization\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=300,\n",
    "    xaxis=dict(title='',showticklabels=False, showgrid=False, zeroline=False),\n",
    "    yaxis=dict(title='',showticklabels=False, showgrid=False, zeroline=False),\n",
    "    title='t-SNE Visualization of \"Other\" (Rare) Car Names'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2963892-8735-4f3a-8c99-d5f3e5d5258d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Observations:\n",
    "The data points are widely scattered throughout the t-SNE plots, indicating limited clear structure or clustering patterns, which suggests the need for additional feature refinement or alternative dimensionality reduction techniques to better capture relationships.\n",
    "\n",
    "* Although t-SNE visualized clusters by association with vehicle performance features, the display of the car name labels themselves show that:\n",
    "  1. There are inconsistencies with how a same make of vehicle, i.e. Volkswagon, is designated as \"VW\"\n",
    "  2. Vehicles' car name meaning can be improved by extracting the first term in the field\n",
    "  3. Further grouping of car name may be performed using fuzzy matching as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b86d6a1-39df-4121-8bf1-97de842dfa0a",
   "metadata": {},
   "source": [
    "#### Feature Engineering: Car Brand review and clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c18b12-988c-430d-b840-1c7e150ec342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first word from 'car name'\n",
    "df['car_brand'] = df['car name'].str.split().str[0]\n",
    "\n",
    "# View the unique brands\n",
    "print(df['car_brand'].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d80f84-e0eb-4276-8f3b-ddce46a430f1",
   "metadata": {},
   "source": [
    "##### Observation\n",
    "* Many misspelled words, so fuzzy matching may be helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c5dc1-d9ff-4b98-8814-8e9631f92934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selective review of extracted car brands\n",
    "df[df['car_brand'] == 'hi']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdbe154-e5e2-47d4-866a-bc046912b452",
   "metadata": {},
   "source": [
    "##### Observations\n",
    "* Internet search results for \"1970 1200d\" revealed that this is a vintage International Harvester: \"Overall, the 1970 International Harvester 1200D is a rare and sought-after vintage pickup truck, prized for its ruggedness, reliability, and nostalgic appeal.\"\n",
    "* This confirms that 'car name' could be valuable in the Vintage Classification of the dataset\n",
    "\n",
    "##### Decision Point\n",
    "* \"hi\" will be renamed \"harvester\", and the car name will remain as it was entered in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6afa7f1-49bd-4b25-a625-46cc0a5a6872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename \"hi\" to \"harvester\"\n",
    "df.loc[df['car_brand'] == 'hi', 'car_brand'] = 'harvester'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aab2e0c-d844-4bef-ad4c-63f938bddb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm update \n",
    "df[df['car_brand'] == 'harvester']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d9181c-2a5a-4cf1-b1a3-97545a22d882",
   "metadata": {},
   "source": [
    "#### Clean-up car brand with mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54078909-99a9-4778-9dbb-ceec940f8b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup typos with mapping of known inconsistencies\n",
    "\n",
    "brand_mapping = {\n",
    "    'chevy': 'chevrolet',\n",
    "    'chevroelt': 'chevrolet',\n",
    "    'vw': 'volkswagen',\n",
    "    'vokswagen': 'volkswagen',\n",
    "    'toyouta': 'toyota',\n",
    "    'maxda': 'mazda',\n",
    "    'mercedes': 'mercedes-benz'\n",
    "}\n",
    "\n",
    "# Apply the mapping to standardize the brands\n",
    "df['car_brand'] = df['car_brand'].replace(brand_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813de446-d1a7-4b08-9177-6248c32d60e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selective review of extracted car brands (capri vs ford vs mercury): Capri produced by Mercury, owned by Ford\n",
    "print(df[df['car_brand'] == 'ford'][['model year', 'car name', 'index', 'car_brand']], \"\\n\")\n",
    "print(df[df['car_brand'] == 'mercury'][['model year', 'car name', 'index', 'car_brand']], \"\\n\")\n",
    "print(df[df['car_brand'] == 'capri'][['model year', 'car name', 'index', 'car_brand']], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc06fd2-7daa-45af-824f-b953f6d62d84",
   "metadata": {},
   "source": [
    "##### Decision Point\n",
    "* \"capri ii\", identified with a \"capri\" car brand would be better classified as a \"mercury\" car brand when queried with other mercury capri vehicles in the data set\n",
    "* Thus, \"mercury\" will replace its brand, while \"capri ii\" will remain as its car name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcacab4-dad7-4625-bb33-d603623c2151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename \"capri\" car brand to \"mercury\"\n",
    "df.loc[df['car_brand'] == 'capri', 'car_brand'] = 'mercury'\n",
    "\n",
    "# Confirm updated car brand\n",
    "df[df['car_brand'] == 'mercury']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af40972-6dd6-424f-8353-f7d9fbce7703",
   "metadata": {},
   "source": [
    "##### Observation\n",
    "* The \"capri ii\" is now categorized under the \"mercury\" car brand. However, further preprocessing of the car name field to extract explicit models (e.g., \"car_model\") will enhance subsequent reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cbd028-c0f0-4776-b5f7-00b1da4657ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review unique brands\n",
    "print(df['car_brand'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438df41-f42d-4c69-b2f5-9bf70a5662e7",
   "metadata": {},
   "source": [
    "##### Observation\n",
    "* The list of car brands are now consistent and succinct\n",
    "* A fuzzy match will ensure there are no surprises between car_brand and car name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9c342b-2c08-4826-bdd7-6d1af8acb549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Function to calculate fuzzy match score\n",
    "def calculate_fuzzy_score(row):\n",
    "    return fuzz.ratio(row['car name'], row['car_brand'])\n",
    "\n",
    "# Create a new DataFrame for testing\n",
    "df_fuzzy_test = df[['car name', 'car_brand']].copy()\n",
    "df_fuzzy_test['fuzzy_score'] = df_fuzzy_test.apply(calculate_fuzzy_score, axis=1)\n",
    "\n",
    "# View potential mismatches\n",
    "df_fuzzy_test_sorted = df_fuzzy_test.sort_values(by='fuzzy_score', ascending=True)\n",
    "\n",
    "# Show mismatches for inspection\n",
    "print(df_fuzzy_test_sorted.head(20))  # Display the lowest scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00101211-5fe1-4132-ba3b-60d6efd76811",
   "metadata": {},
   "source": [
    "##### Observation\n",
    "* The fuzzy match shows that car_brand can be reliabily used for visualization and clustering analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6043186-f50f-4a89-bf50-f174ced85352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review count of unique car brands\n",
    "\n",
    "df['car_brand'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9019ab80-3e3d-44bf-8b00-3448662c9463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review other single-record car brands\n",
    "print(df[df['car_brand'] == 'nissan'], \"\\n\")\n",
    "print(\"---\")\n",
    "print(\"\\n\",df[df['car_brand'] == 'triumph'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84866c56-ba76-479c-8b34-ccc6e4740824",
   "metadata": {},
   "source": [
    "##### Findings\n",
    "While Triumph is a single-record brand, similar to International Harvester, the Nissan Stanza is owned by Datsun and should be reviewed to determine whether it should be reassigned to this brand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528bbb7c-7bbb-4958-802b-6fbb804a8128",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['car_brand'] == 'datsun']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a7cc80-9f90-4c30-b5dc-027db6ca1783",
   "metadata": {},
   "source": [
    "##### Decision Point\n",
    "There are no \"Stanza\" models in the above 'datsun\" filtered car brand dataset. According to an internet search, while Nissan vehicles were sold under the Datsun brand until 1983, the Datsun brand was subsequently discontinued, and the Stanza XE was rebranded as the Nissan Stanza XE. Therefore, the single \"Nissan\" record will remain unchanged in this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c408f-7595-4a00-bc1e-5437c9fa3a2c",
   "metadata": {},
   "source": [
    "#### t-SNE_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6777319-2692-40dc-834d-57535fa0d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "\n",
    "# Suppress selected Future Warning\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas.*\",\n",
    "    category=FutureWarning\n",
    ")\n",
    "\n",
    "\n",
    "# Encode categorical car brands\n",
    "le = LabelEncoder()\n",
    "df['car_brand_encoded'] = le.fit_transform(df['car_brand'])\n",
    "\n",
    "# Drop unnecessary columns and prepare features\n",
    "features = df.drop(['car name', 'car_brand'], axis=1)\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(features)-1))\n",
    "features_tsne = tsne.fit_transform(features)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "tsne_df = pd.DataFrame(features_tsne, columns=['t-SNE 1', 't-SNE 2'])\n",
    "tsne_df['car_brand'] = df['car_brand']\n",
    "tsne_df['car_brand_encoded'] = df['car_brand_encoded']\n",
    "\n",
    "# Plot the t-SNE visualization\n",
    "fig = px.scatter(tsne_df, x='t-SNE 1', y='t-SNE 2', \n",
    "                 color='car_brand', \n",
    "                 hover_data=['car_brand'])\n",
    "\n",
    "# Update layout for cleaner visualization\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=300,\n",
    "    xaxis=dict(title='',showticklabels=False, showgrid=False, zeroline=False),\n",
    "    yaxis=dict(title='',showticklabels=False, showgrid=False, zeroline=False),\n",
    "    title='t-SNE Visualization of Car Brands'\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60af0460-cba7-4bc5-aab6-28380e85582f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Observation\n",
    "Clustering based on car brand and vintage classification still seem to be obscured by performance features. It seems reasonable therefore that *removing* these features will yield more coherent groupings (as it pertains to **Vintage Classification** than applying PCA for dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc2934b-e835-4fcc-8530-f3fb15f0b70f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Hypothesis: Vintage Classification\n",
    "* Vintage classification is primarily determined by the interaction between car name and model year.\n",
    "\n",
    "**Null Hypothesis (ùêª0):**\n",
    "* The interaction between car name and model year sufficiently explains vintage classification *without* the need for additional feature (i.e. *[performance features](#The_overview_above_shows:)*) transformations such as PCA.\n",
    "\n",
    "* Implication:\n",
    "    * Clustering without PCA (using car name and model year along with other original features) will yield clusters of similar quality as clustering after PCA.\n",
    "\n",
    "    * PCA will not significantly improve cluster separability or performance because the most relevant variance is already captured by car name and model year.\n",
    "\n",
    "**Alternative Hypothesis (ùêªùëé):**\n",
    "* PCA will reveal latent features that significantly improve vintage classification beyond the interaction of car name and model year alone.\n",
    "\n",
    "* Implication:\n",
    "    * Clustering after PCA will produce significantly better-defined clusters (e.g., higher silhouette scores, better cluster separability).\n",
    "    * Other features or combinations of features (e.g., displacement, acceleration, or transformed horsepower) contain important latent information relevant to vintage classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d652fc-ba7e-451a-81d1-39d03959366a",
   "metadata": {},
   "source": [
    "### KMeans *without* vs *with Performance Features* vs *PCA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d0ae1-10d7-4bb4-a681-bd28525df9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin with only car brand and model year for 3-cluster scoring using Silhouette Scores\n",
    "# \"Core\": without performance features\n",
    "# \"Full\": with performance features\n",
    "# \"PCA\": with performance features using PCA\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "\n",
    "# Suppress specific KMeans warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"KMeans is known to have a memory leak on Windows with MKL\")\n",
    "\n",
    "# Label encode 'car_brand'\n",
    "le = LabelEncoder()\n",
    "df['car_brand_encoded'] = le.fit_transform(df['car_brand'])\n",
    "\n",
    "# Core\n",
    "# Drop performance features and keep 'car_brand_encoded'\n",
    "features_core = df[['model year', 'car_brand_encoded']]\n",
    "\n",
    "# Fit KMeans \n",
    "kmeans_core = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels_core = kmeans_core.fit_predict(features_core)\n",
    "\n",
    "# Evaluate cluster quality\n",
    "silhouette_core = silhouette_score(features_core, labels_core)\n",
    "print(f'Silhouette Score (Core Features - Null Hypothesis): {silhouette_core}')\n",
    "\n",
    "# Include all features, testing null hypothesis \n",
    "features_full = df[['model year', 'car_brand_encoded', 'weight', 'displacement']]\n",
    "\n",
    "# Fit KMeans\n",
    "kmeans_full = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels_full = kmeans_full.fit_predict(features_full)\n",
    "\n",
    "# Evaluate cluster quality\n",
    "silhouette_full = silhouette_score(features_full, labels_full)\n",
    "print(f'Silhouette Score (Full Features with Non-Performance Metrics): {silhouette_full}')\n",
    "\n",
    "# Compare alternative hypothesis with PCA + KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to features before running KMeans\n",
    "pca = PCA(n_components=2)  # Reduce to 2 dimensions\n",
    "#features_pca = pca.fit_transform(features_core) Silhouette Score (PCA Features - Alternative Hypothesis): 0.4035467744569793\n",
    "features_pca = pca.fit_transform(features_full)\n",
    "\n",
    "# Fit KMeans on PCA-reduced features\n",
    "kmeans_pca = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels_pca = kmeans_pca.fit_predict(features_pca)\n",
    "\n",
    "# Evaluate cluster quality\n",
    "silhouette_pca = silhouette_score(features_pca, labels_pca)\n",
    "print(f'Silhouette Score (PCA Features - Alternative Hypothesis): {silhouette_pca}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ca4b8-b6fe-4ccb-9ca8-cc61a7e8ab15",
   "metadata": {},
   "source": [
    "##### Silhouette Score\n",
    "\n",
    "* Evaluation of the *quality of clustering* as represented by the Silhouette Score: \n",
    "\n",
    "    1. Cluster Validity: A higher score indicates that clusters are well-formed and distinct.\n",
    "    2. Optimal Number of Clusters: The silhouette score can help reveal and identify the ideal number of clusters.\n",
    "    3. Cluster Interpretability: Assess whether the clustering reflects meaningful patterns in the data (e.g., does it align with vintage classification?).\n",
    " \n",
    "**Findings**\n",
    "1. Core Features (Null Hypothesis):\n",
    "\n",
    "* Silhouette Score: 0.4035\n",
    "* This reflects a moderate clustering quality, suggesting that while model year and car brand alone can form clusters, they lack the nuance to capture fully distinct vintage classifications.\n",
    "\n",
    "2. Full Features (Non-Performance Metrics):\n",
    "\n",
    "* Silhouette Score: 0.5872\n",
    "* A substantial improvement, showing that adding weight and displacement leads to more cohesive and well-separated clusters.\n",
    "* These features seem to carry valuable information about vehicle design or era-related trends, which help refine vintage classification.\n",
    "\n",
    "3. PCA on Full Features (Alternative Hypothesis):\n",
    "\n",
    "* Silhouette Score: 0.5877\n",
    "* Almost identical to the Full Features model without PCA.\n",
    "* This suggests that PCA successfully reduces dimensionality while preserving important patterns, but it doesn‚Äôt improve cluster separability significantly beyond the raw features.\n",
    "\n",
    "##### Next Step\n",
    "* Compare results with scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0bd8e9-4167-4734-9ca2-60f8f2a064e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling all 3 models to compare\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale Core Features\n",
    "features_core_scaled = scaler.fit_transform(features_core)\n",
    "\n",
    "# Scale Full Features\n",
    "features_full_scaled = scaler.fit_transform(features_full)\n",
    "\n",
    "# Step 1: PCA on Scaled Full Features\n",
    "pca = PCA(n_components=3)  \n",
    "features_pca_scaled = pca.fit_transform(features_full_scaled)\n",
    "\n",
    "# Step 2: Fit KMeans and Evaluate Silhouette Scores\n",
    "print('Scaled Silhouette Scoring:')\n",
    "# Core Features Model\n",
    "kmeans_core = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels_core = kmeans_core.fit_predict(features_core_scaled)\n",
    "silhouette_core = silhouette_score(features_core_scaled, labels_core)\n",
    "print(f'Silhouette Score (Core Features - Scaled): {silhouette_core}')\n",
    "\n",
    "# Full Features Model\n",
    "kmeans_full = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels_full = kmeans_full.fit_predict(features_full_scaled)\n",
    "silhouette_full = silhouette_score(features_full_scaled, labels_full)\n",
    "print(f'Silhouette Score (Full Features - Scaled): {silhouette_full}')\n",
    "\n",
    "# PCA Features Model\n",
    "kmeans_pca = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "labels_pca = kmeans_pca.fit_predict(features_pca_scaled)\n",
    "silhouette_pca = silhouette_score(features_pca_scaled, labels_pca)\n",
    "print(f'Silhouette Score (PCA Features - Scaled): {silhouette_pca}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181c8db6-4350-4515-af9a-839024cf286f",
   "metadata": {},
   "source": [
    "##### Finding\n",
    "* Core Features show the highest silhouette score when compared to full features with and without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87887b01-0de7-487a-886c-9899a7d023cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the 3 models together\n",
    "\n",
    "import plotly.express as px\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Suppress the FutureWarning from Plotly\n",
    "warnings.filterwarnings(\"ignore\", message=\"When grouping with a length-1 list-like\")\n",
    "\n",
    "# Decode 'car_brand_encoded' back to original 'car_brand'\n",
    "df['car_brand'] = le.inverse_transform(df['car_brand_encoded'])\n",
    "\n",
    "# t-SNE for Core Features\n",
    "tsne_core = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "features_tsne_core = tsne_core.fit_transform(features_core)\n",
    "tsne_df_core = pd.DataFrame(features_tsne_core, columns=['t-SNE 1', 't-SNE 2'])\n",
    "tsne_df_core['car brand'] = df.loc[features_core.index, 'car_brand']\n",
    "tsne_df_core['cluster'] = labels_core\n",
    "tsne_df_core['model'] = 'Core Features'\n",
    "tsne_df_core['index'] = features_core.index\n",
    "tsne_df_core['car name'] = df.loc[features_core.index, 'car name']\n",
    "tsne_df_core['model year'] = df.loc[features_core.index, 'model year']\n",
    "\n",
    "# t-SNE for Full Features\n",
    "tsne_full = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "features_tsne_full = tsne_full.fit_transform(features_full)\n",
    "tsne_df_full = pd.DataFrame(features_tsne_full, columns=['t-SNE 1', 't-SNE 2'])\n",
    "tsne_df_full['car brand'] = df.loc[features_full.index, 'car_brand']\n",
    "tsne_df_full['cluster'] = labels_full\n",
    "tsne_df_full['model'] = 'Full Features'\n",
    "tsne_df_full['index'] = features_full.index\n",
    "tsne_df_full['car name'] = df.loc[features_full.index, 'car name']\n",
    "tsne_df_full['model year'] = df.loc[features_full.index, 'model year']\n",
    "\n",
    "# t-SNE for PCA Features\n",
    "tsne_pca = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "features_tsne_pca = tsne_pca.fit_transform(features_pca)\n",
    "tsne_df_pca = pd.DataFrame(features_tsne_pca, columns=['t-SNE 1', 't-SNE 2'])\n",
    "tsne_df_pca['car brand'] = df.loc[features_full.index, 'car_brand']\n",
    "tsne_df_pca['cluster'] = labels_pca\n",
    "tsne_df_pca['model'] = 'PCA Features'\n",
    "tsne_df_pca['index'] = features_full.index\n",
    "tsne_df_pca['car name'] = df.loc[features_full.index, 'car name']\n",
    "tsne_df_pca['model year'] = df.loc[features_full.index, 'model year']\n",
    "\n",
    "# Combine t-SNE DataFrames\n",
    "tsne_combined = pd.concat([tsne_df_core, tsne_df_full, tsne_df_pca])\n",
    "\n",
    "# Plot with facet by model and custom hover\n",
    "fig = px.scatter(tsne_combined, \n",
    "                 x='t-SNE 1', \n",
    "                 y='t-SNE 2', \n",
    "                 color='cluster', \n",
    "                 facet_col='model', \n",
    "                 title='t-SNE Comparison of Core vs Full vs Full with PCA')\n",
    "\n",
    "# Customize hovertemplate\n",
    "fig.update_traces(\n",
    "    hovertemplate='<br>Car Brand: %{customdata[0]}<br>Car Name: %{customdata[1]}<br>Model Year: %{customdata[2]}<br>Cluster: %{customdata[3]}<extra></extra>',\n",
    "    customdata=tsne_combined[['car brand', 'car name', 'model year', 'cluster']].to_numpy()\n",
    ")\n",
    "\n",
    "# Suppress t-SNE axis labels for all facets\n",
    "fig.update_xaxes(title='', showticklabels=False, showgrid=False, zeroline=False, matches='x')\n",
    "fig.update_yaxes(title='', showticklabels=False, showgrid=False, zeroline=False, matches='y')\n",
    "\n",
    "# Control dimensions\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=800,\n",
    "    height=400,\n",
    "    coloraxis_showscale=False \n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ef1397-6520-43fb-9d63-ee6c876e0add",
   "metadata": {},
   "source": [
    "##### General Observations\n",
    "* It seems PCA and the Full Features without PCA are very much alike; the shape of these models also resemble the original t-SNE preview and review visualizations\n",
    "* Core Features model seems to reflect clear, distinct clusters, with apparent separation between clusters; this seem to suggest meaningful differences between data points\n",
    "* An evaluation of optimal number of clusters should be examined next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bda7b31-ec91-4988-aaea-e1b42ce9b68f",
   "metadata": {},
   "source": [
    "#### ElbowPlots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06497181-be18-4547-a06c-bc4634eef356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review optimal clusters with Elbow Plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Define a function to create the elbow plot\n",
    "def plot_elbow(features, title):\n",
    "    inertia_values = []\n",
    "    cluster_range = range(1, 11)  # Test 1 to 10 clusters\n",
    "\n",
    "    for k in cluster_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(features)\n",
    "        inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "    # Plot the elbow plot\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.plot(cluster_range, inertia_values, marker='o', linestyle='--')\n",
    "    plt.title(f'{title}', fontsize=10)\n",
    "    plt.xlabel('Number of Clusters (k)', fontsize=8)\n",
    "    plt.ylabel('Inertia (WCSS)', fontsize=8)\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Elbow Plot for Core Features (Only Car Brand and Model Year)\n",
    "plot_elbow(features_core, 'Core Features (Car Brand + Model Year)')\n",
    "\n",
    "# Elbow Plot for Full Features (Model Year + Car Brand + Non-Performance Metrics)\n",
    "plot_elbow(features_full, 'Full Features')\n",
    "\n",
    "# Elbow Plot for PCA Features (Dimensionality Reduced Features)\n",
    "plot_elbow(features_pca, 'PCA Features (Full Features Reduced to 2D)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3603c936-483a-4bb5-984f-a0ebdd17ed49",
   "metadata": {},
   "source": [
    "##### Findings\n",
    "* All 3 models agree that k = 2 is the optimum number of clusters\n",
    "* Re-run KMeans where k = 2 for all 3 models\n",
    "* Also re-run as k=2 to compare scaling for all 3 models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd3bf2-d545-4c8d-9e5f-3c5cdec90a09",
   "metadata": {},
   "source": [
    "#### Clustering iterations with 2 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ad61c-0995-4bd1-80ad-e0838479f52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "\n",
    "# Suppress specific KMeans warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"KMeans is known to have a memory leak on Windows with MKL\")\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Encodes categorical variables and prepares data for clustering.\"\"\"\n",
    "    # Label encode 'car_brand'\n",
    "    le = LabelEncoder()\n",
    "    df['car_brand_encoded'] = le.fit_transform(df['car_brand'])\n",
    "    return df\n",
    "\n",
    "def get_features(df):\n",
    "    \"\"\"Define the feature sets for clustering.\"\"\"\n",
    "    # Core features: minimal set\n",
    "    features_core = df[['model year', 'car_brand_encoded']]\n",
    "    # Full features: include additional metrics\n",
    "    features_full = df[['model year', 'car_brand_encoded', 'weight', 'displacement']]\n",
    "    return features_core, features_full\n",
    "\n",
    "def calculate_silhouette_scores(features_core, features_full):\n",
    "    \"\"\"Calculates silhouette scores for different feature sets, scaling, and cluster counts.\"\"\"\n",
    "    results = []\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Core Features: Scaled and Unscaled\n",
    "    features_core_scaled = scaler.fit_transform(features_core)\n",
    "    for scaling, features in zip([\"Unscaled\", \"Scaled\"], [features_core, features_core_scaled]):\n",
    "        for k in [2, 3]:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            labels = kmeans.fit_predict(features)\n",
    "            silhouette = silhouette_score(features, labels)\n",
    "            results.append({\n",
    "                \"Feature Set\": \"Core Features\",\n",
    "                \"Scaling\": scaling,\n",
    "                \"Clusters\": k,\n",
    "                \"Silhouette Score\": silhouette\n",
    "            })\n",
    "\n",
    "    # Full Features: Scaled and Unscaled\n",
    "    features_full_scaled = scaler.fit_transform(features_full)\n",
    "    for scaling, features in zip([\"Unscaled\", \"Scaled\"], [features_full, features_full_scaled]):\n",
    "        for k in [2, 3]:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            labels = kmeans.fit_predict(features)\n",
    "            silhouette = silhouette_score(features, labels)\n",
    "            results.append({\n",
    "                \"Feature Set\": \"Full Features\",\n",
    "                \"Scaling\": scaling,\n",
    "                \"Clusters\": k,\n",
    "                \"Silhouette Score\": silhouette\n",
    "            })\n",
    "\n",
    "    # PCA Features: Scaled and Unscaled\n",
    "    for scaling, features in zip([\"Unscaled\", \"Scaled\"], [features_full, features_full_scaled]):\n",
    "        pca = PCA(n_components=2)\n",
    "        features_pca = pca.fit_transform(features)\n",
    "        for k in [2, 3]:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            labels = kmeans.fit_predict(features_pca)\n",
    "            silhouette = silhouette_score(features_pca, labels)\n",
    "            results.append({\n",
    "                \"Feature Set\": \"PCA Features\",\n",
    "                \"Scaling\": scaling,\n",
    "                \"Clusters\": k,\n",
    "                \"Silhouette Score\": silhouette\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def print_results(df_silhouette):\n",
    "    \"\"\"Prints silhouette scores in the desired format.\"\"\"\n",
    "    for scaling in [\"Unscaled\", \"Scaled\"]:\n",
    "        print(f\"2-Cluster, {scaling} Silhouette Scoring:\")\n",
    "        filtered = df_silhouette[(df_silhouette[\"Clusters\"] == 2) & (df_silhouette[\"Scaling\"] == scaling)]\n",
    "        for _, row in filtered.iterrows():\n",
    "            print(f\"Silhouette Score ({row['Feature Set']} - {scaling}): {row['Silhouette Score']}\")\n",
    "        print()\n",
    "\n",
    "def main(df):\n",
    "    # Step 1: Preprocess Data\n",
    "    df = preprocess_data(df)\n",
    "\n",
    "    # Step 2: Get Feature Sets\n",
    "    features_core, features_full = get_features(df)\n",
    "\n",
    "    # Step 3: Calculate Silhouette Scores\n",
    "    df_silhouette = calculate_silhouette_scores(features_core, features_full)\n",
    "\n",
    "    # Step 4: Print Results\n",
    "    print_results(df_silhouette)\n",
    "\n",
    "    # Step 5: Return DataFrame for further analysis\n",
    "    return df_silhouette\n",
    "\n",
    "# Run the models: Full vs Core, Scaled vs Unscaled, Produce corresponding silhouette scores\n",
    "df_silhouette = main(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720636de-4395-481b-ada5-ccce6ec7abbb",
   "metadata": {},
   "source": [
    "### Statistical Benchmarking and Cross-Context Validation \n",
    "* While silhouette scores are meaningful within their immediate contexts for comparison, i.e. 2-cluster vs 3-cluster, scaled vs unscaled, we can more meaningfully compare across models using statistical comparisons of the clustering models\n",
    "* Coefficient of Variation (CV) and Relative Range can be applied as key metrics to provide a global measure of clustering performance and stability.\n",
    "* Although all the [elbow plots](#ElbowPlots) of the three models have unanimously identified having 2 clusters as optimal, like the sihouette score, the elbow point is not an *absolute metric*.\n",
    "    * The elbow plot provides guidance rather than definitive answers in the absolute sense.\n",
    "    * The results from the elbow plots must therefore be interpreted in conjunction with other metrics like stability and reliability metrics, as well as domain knowledge. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0876dd-4d0c-4199-a8a2-e2b58c009472",
   "metadata": {},
   "source": [
    "### Stability and Reliability Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed59b349-042b-4877-8ffd-2d1cc490ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient of Variation: Quantify relative variability across model scenarios\n",
    "\n",
    "# CV by Feature Set and Scaling (Aggregate Clusters)\n",
    "cv_results_feature_scaling = df_silhouette.groupby(['Feature Set', 'Scaling'])['Silhouette Score'].apply(\n",
    "    lambda x: (np.std(x, ddof=1) / np.mean(x)) * 100\n",
    ")\n",
    "print(\"\\nCoefficient of Variation (CV) by Feature Set and Scaling:\")\n",
    "print(cv_results_feature_scaling)\n",
    "\n",
    "# CV by Feature Set and Clusters (Aggregate Scaling)\n",
    "cv_results_feature_clusters = df_silhouette.groupby(['Feature Set', 'Clusters'])['Silhouette Score'].apply(\n",
    "    lambda x: (np.std(x, ddof=1) / np.mean(x)) * 100\n",
    ")\n",
    "print(\"\\nCoefficient of Variation (CV) by Feature Set and Clusters:\")\n",
    "print(cv_results_feature_clusters)\n",
    "\n",
    "# CV by Scaling and Clusters (Aggregate Feature Set)\n",
    "cv_results_scaling_clusters = df_silhouette.groupby(['Scaling', 'Clusters'])['Silhouette Score'].apply(\n",
    "    lambda x: (np.std(x, ddof=1) / np.mean(x)) * 100\n",
    ")\n",
    "print(\"\\nCoefficient of Variation (CV) by Scaling and Clusters:\")\n",
    "print(cv_results_scaling_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d09717-c810-4538-8cf2-58581376be26",
   "metadata": {},
   "source": [
    "#### Findings: Coefficient of Variation: Stability / Reliability\n",
    "\n",
    "* The CV analysis demonstrates that Full Features with 3 Clusters is a highly variable model and therefore less reliable for drawing robust insights.\n",
    "* The **Core Features with 3 Clusters has the lowest CV (3.07%)**:\n",
    "    * This indicates that the silhouette scores for this configuration are highly consistent across different contexts.\n",
    "    * It also suggests that the clustering results are stable, making this model more reliable for drawing conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ae8fe-4a73-45c8-a4fa-cddbbe7c7a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range and Relative Range by Feature Set and Scaling\n",
    "\n",
    "range_feature_scaling = df_silhouette.groupby(['Feature Set', 'Scaling'])['Silhouette Score'].agg(\n",
    "    Range=lambda x: x.max() - x.min(),\n",
    "    Relative_Range=lambda x: ((x.max() - x.min()) / x.max()) * 100 if x.max() > 0 else np.nan\n",
    ")\n",
    "\n",
    "print(\"\\nRange and Relative Range by Feature Set and Scaling:\")\n",
    "print(range_feature_scaling)\n",
    "\n",
    "# Range and Relative Range by Feature Set and Clusters\n",
    "range_feature_clusters = df_silhouette.groupby(['Feature Set', 'Clusters'])['Silhouette Score'].agg(\n",
    "    Range=lambda x: x.max() - x.min(),\n",
    "    Relative_Range=lambda x: ((x.max() - x.min()) / x.max()) * 100 if x.max() > 0 else np.nan\n",
    ")\n",
    "\n",
    "print(\"\\nRange and Relative Range by Feature Set and Clusters:\")\n",
    "print(range_feature_clusters)\n",
    "\n",
    "# Range and Relative Range by Scaling and Clusters\n",
    "range_scaling_clusters = df_silhouette.groupby(['Scaling', 'Clusters'])['Silhouette Score'].agg(\n",
    "    Range=lambda x: x.max() - x.min(),\n",
    "    Relative_Range=lambda x: ((x.max() - x.min()) / x.max()) * 100 if x.max() > 0 else np.nan\n",
    ")\n",
    "\n",
    "print(\"\\nRange and Relative Range by Scaling and Clusters:\")\n",
    "print(range_scaling_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f122b56e-eb56-489b-bb17-a8fc16e1b995",
   "metadata": {},
   "source": [
    "##### Findings:  Range and Relative Range\n",
    "* By Scaling: Core Features (Scaled) has the lowest relative range among scaled sets (7.30), indicating *high stability* in scaled data. Core Features (Unscaled) has the lowest relative range among unscaled sets (27.49), showing greater stability in raw form compared to other unscaled features.\n",
    "* By Clusters: The most stable cluster configuration appears for Core Features with 3 Clusters (4.25), which has the lowest relative range overall.\n",
    "* By Scaling and Clusters: Unscaled with 2 Clusters has the lowest relative range (12.46), suggesting that unscaled features are more stable with fewer clusters.\n",
    "\n",
    "Therefore, the most stable overall configuration is Core Features with 3 Clusters. Most stable scaled data is Core Features (Scaled). Most stable unscaled data is Core Features (Unscaled) (27.49). From these, **Core Features with 3 Clusters** is the most stable and reliable configuration overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ca564c-635e-4a49-b78a-abbd1ee12e66",
   "metadata": {},
   "source": [
    "#### Visualization of Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e950fcfe-e983-4908-a814-271111b3396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add Adjusted Silhouette Score (1 - x)\n",
    "df_silhouette['Adjusted Score'] = 1 - df_silhouette['Silhouette Score']\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Subplot 1: Bar Chart\n",
    "sns.barplot(ax=axes[0], x='Feature Set', y='Adjusted Score', data=df_silhouette, ci=None)\n",
    "axes[0].set_title('Average Reliability/Stability Score by Feature Set')\n",
    "axes[0].set_ylabel('Adjusted Silhouette Score (1 - Silhouette Score)')\n",
    "axes[0].set_xlabel('Feature Set')\n",
    "\n",
    "# Subplot 2: Heatmap\n",
    "heatmap_data = df_silhouette.pivot_table(\n",
    "    index='Feature Set', columns='Clusters', values='Adjusted Score', aggfunc='mean'\n",
    ")\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.2f', cmap='coolwarm', ax=axes[1])\n",
    "axes[1].set_title('Reliability/Stability Measure: Feature Set x Clusters')\n",
    "axes[1].set_xlabel('Number of Clusters')\n",
    "axes[1].set_ylabel('Feature Set')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the combined figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1519b7c-e886-4304-a82d-2b88274daceb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Findings: Magnitude of variability relative to the highest silhouette score in each context\n",
    "\n",
    "* Core Features with 3 Clusters consistently show the *lowest* Relative Range across all groupings, further validating their **stability and reliability**.\n",
    "* Full Features with 3 Clusters consistently have the *highest* Relative Range, indicating **significant instability**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d969e4b-18db-4677-8f04-0dd17d4acaba",
   "metadata": {},
   "source": [
    "### Hypothesis Testing Results, Conclusion & Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca6d363-5144-4950-b3a6-32e62e773f5d",
   "metadata": {},
   "source": [
    "Null Hypothesis (ùêª0):\n",
    "* The interaction between car name and model year sufficiently explains vintage classification without PCA.\n",
    "\n",
    "Supporting Evidence:\n",
    "\n",
    "1. Core Features with 3 Clusters consistently outperforms other feature sets in CV (3.07%) and Relative Range (4.25%).\n",
    "2. Low silhouette score variability confirms that adding more dimensions with Full Feature Set, or reducing dimensionality using PCA, introduces noise rather than clarity.\n",
    "\n",
    "Alternative Hypothesis (ùêªùëé):\n",
    "* PCA reveals latent features that significantly improve vintage classification.\n",
    "\n",
    "Counter Evidence:\n",
    "\n",
    "1. PCA Features with 2 Clusters improve relative variability over most setups, but PCA features introduce higher complexity without outperforming.\n",
    "2. No configuration after PCA significantly outperforms Core Clusters for Vintage scoring.\n",
    "\n",
    "**Conclusion & Recommendations**\n",
    "\n",
    "*Support for Null Hypothesis (ùêª0):*\n",
    "\n",
    "* Clustering based on car name and model year is sufficient for stable and reliable vintage classification.\n",
    "* Core Features with 3 Clusters demonstrates lowest variability and most consistent clustering behavior.\n",
    "\n",
    "*1. Conclusion:* The Core Features with 3 Clusters (see Cluster Diagram and Data Table below) can be leveraged to extract valuable insights about the cars, grouping them to more effectively target the audience. Further analysis with domain experts will be necessary to achieve this.\n",
    "\n",
    "*2. Recommendations:*\n",
    "* Start with the simpler Core Features, avoiding unnecessary transformations.\n",
    "    * PCA may introduce latent dimensions, but for this dataset, performance improvement was marginal with greater variability in cluster quality.\n",
    "    * Domain expert(s) should review the 3-Cluster report to apply labels onto the dataset; these labels then can be further leveraged in supervised learning (see below).\n",
    "* In collaboration with domain expert(s):\n",
    "    1. Further feature engineer the dataset, i.e. the vehicles' model using 'car name' to enhance clustering by car_brand, year, and car_model;  performance features (i.e. horespower) should be reviewed and validated as reliable basis for classifying \"Vintage\" status (see below). \n",
    "    2. Further iterations in unsupervised learning may be pursued, exploring the use of multi-model ensemble learning.\n",
    "    3. Supervised learning can be used in lieu of unsupervised learning if Vintage Classification relies heavily on domain expert labelling. \n",
    "    4. Use of Vintage Classification can be further discussed as it pertains to binary (i.e. Vintage vs Non-Vintage) versus multiple classes (i.e. Classic vs Retro vs Modern vs Emerging Vintage). \n",
    "    5. Vintage Classification can be explored as a continuous score using a regression-based approach to combine weighted factors. \n",
    "    6. The model can be leveraged to target audience segmentation: Group potential buyers by their affinity for specific vintage categories. \n",
    "    7. The model can be leveraged for marketing insights: Identify which features drive perceptions of vintage value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d20378a-b246-4a05-bbf3-45835239dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Suppress the FutureWarning from Plotly\n",
    "warnings.filterwarnings(\"ignore\", message=\"When grouping with a length-1 list-like\")\n",
    "\n",
    "# Decode 'car_brand_encoded' back to original 'car_brand'\n",
    "df['car_brand'] = le.inverse_transform(df['car_brand_encoded'])\n",
    "\n",
    "# t-SNE for Core Features\n",
    "tsne_core = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "features_tsne_core = tsne_core.fit_transform(features_core)\n",
    "tsne_df_core = pd.DataFrame(features_tsne_core, columns=['t-SNE 1', 't-SNE 2'])\n",
    "tsne_df_core['car brand'] = df.loc[features_core.index, 'car_brand']\n",
    "tsne_df_core['cluster'] = labels_core\n",
    "tsne_df_core['model'] = 'Core Features'\n",
    "tsne_df_core['index'] = features_core.index\n",
    "tsne_df_core['car name'] = df.loc[features_core.index, 'car name']\n",
    "tsne_df_core['model year'] = df.loc[features_core.index, 'model year']\n",
    "\n",
    "# Plot for Core Features only\n",
    "fig = px.scatter(tsne_df_core, \n",
    "                 x='t-SNE 1', \n",
    "                 y='t-SNE 2', \n",
    "                 color='cluster', \n",
    "                 title='Vehicle by Clusters')\n",
    "\n",
    "# Customize hovertemplate\n",
    "fig.update_traces(\n",
    "    hovertemplate='<br>Car Brand: %{customdata[0]}<br>Car Name: %{customdata[1]}<br>Model Year: %{customdata[2]}<br>Cluster: %{customdata[3]}<extra></extra>',\n",
    "    customdata=tsne_df_core[['car brand', 'car name', 'model year', 'cluster']].to_numpy()\n",
    ")\n",
    "\n",
    "# Suppress elements\n",
    "fig.for_each_annotation(lambda a: a.update(text=''))\n",
    "fig.update_xaxes(title='', showticklabels=False, showgrid=False, zeroline=False)\n",
    "fig.update_yaxes(title='', showticklabels=False, showgrid=False, zeroline=False)\n",
    "fig.update_layout(autosize=False, width=800, height=400, showlegend=False, coloraxis_showscale=False)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75930124-d0e6-479f-a72f-0bdb89671d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and Sort results\n",
    "\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Merge tsne_df_core with original df on 'index'\n",
    "merged_df = tsne_df_core.merge(\n",
    "    df[['index', 'mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration']],\n",
    "    on='index',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Filter out unwanted columns\n",
    "data_table_df = merged_df.drop(columns=['t-SNE 1', 't-SNE 2', 'model', 'index'])\n",
    "\n",
    "# Cast numeric columns to integers\n",
    "filtered_data = data_table_df.copy()\n",
    "numeric_columns = ['cluster', 'model year', 'mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration']\n",
    "filtered_data[numeric_columns] = filtered_data[numeric_columns].astype(int)\n",
    "\n",
    "# Dropdown for sorting columns\n",
    "sort_column_dropdown = widgets.Dropdown(\n",
    "    options=filtered_data.columns.tolist(),\n",
    "    description='Sort by:',\n",
    "    value=filtered_data.columns[0]\n",
    ")\n",
    "\n",
    "# Dropdown for sort order\n",
    "sort_order_dropdown = widgets.Dropdown(\n",
    "    options=['Ascending', 'Descending'],\n",
    "    description='Order:',\n",
    "    value='Ascending'\n",
    ")\n",
    "\n",
    "# Filters for numeric columns with extended value display width\n",
    "filters = {}\n",
    "for column in numeric_columns:\n",
    "    min_val, max_val = filtered_data[column].min(), filtered_data[column].max()\n",
    "    filters[column] = widgets.IntRangeSlider(\n",
    "        value=[min_val, max_val],\n",
    "        min=min_val,\n",
    "        max=max_val,\n",
    "        step=1,\n",
    "        description=column,\n",
    "        continuous_update=False,\n",
    "        layout=widgets.Layout(width='600px')  # Extend slider width\n",
    "    )\n",
    "\n",
    "# Button to apply filters and sorting\n",
    "apply_button = widgets.Button(\n",
    "    description='Apply Filters & Sort',\n",
    "    button_style='success'\n",
    ")\n",
    "\n",
    "# Output widget to display the filtered table\n",
    "output = widgets.Output()\n",
    "\n",
    "# Function to update the table based on filters and sorting\n",
    "def update_table(_):\n",
    "    df = filtered_data.copy()\n",
    "    # Apply numeric filters\n",
    "    for column, slider in filters.items():\n",
    "        df = df[(df[column] >= slider.value[0]) & (df[column] <= slider.value[1])]\n",
    "    # Apply sorting\n",
    "    sort_order = True if sort_order_dropdown.value == 'Ascending' else False\n",
    "    df = df.sort_values(by=sort_column_dropdown.value, ascending=sort_order)\n",
    "    # Display the table\n",
    "    with output:\n",
    "        output.clear_output(wait=True)\n",
    "        display(df)\n",
    "\n",
    "# Attach the function to the apply button\n",
    "apply_button.on_click(update_table)\n",
    "\n",
    "# Display all widgets and the output\n",
    "widgets_ui = widgets.VBox(\n",
    "    [sort_column_dropdown, sort_order_dropdown, *filters.values(), apply_button, output]\n",
    ")\n",
    "display(widgets_ui)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myVenv)",
   "language": "python",
   "name": "myenv-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
